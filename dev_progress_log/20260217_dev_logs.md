# Universal LLM Batch Generation Framework — Progress Summary

> Status: **Pipelines 0–6 completed, tested, and real-execution verified end-to-end** ✅  
> Date: **2026-02-17** (Asia/Tokyo)  
> Repo Structure: **Orchestration-only pipelines + reusable core/IO/utils/LLM modules**  
> Output Guarantee: **Deterministic artifacts + strict schema validation + optional judge-gated acceptance**

---

## 0) What This Framework Is

A **universal, task-agnostic LLM batch generation framework** that:

- Ingests **one** tabular file (`csv/psv/tsv/xlsx`)
- Builds deterministic **WorkItems** (row-wise or grouped)
- Runs LLM generation with **strict schema validation**
- Optionally runs a **judge** prompt with auto-retry
- Exports results into analysis-ready formats (`jsonl` + `psv`)
- Writes a report (`md` + `html`) that summarizes run health and quality

The framework is designed for:
- **Reproducibility** (hashing, stable ordering, stable serialization)
- **Auditability** (artifact-first, manifest-first)
- **Scalability** (parallel execution, cache hits, group-context de-dup)
- **Forward-compatibility** (schema evolution + prompt evolution without breaking downstream logic)

---

## 1) Architecture and Code Layout

### 1.1 High-level flow (Pipelines 0–6)

```
configs/parameters.yaml
|
v
[0] ensure schema/llm_schema.py  (runtime validation schema)
|
v
[1] ensure schema/llm_schema.txt (prompt-ready JSON schema text)
|
v
[2] ingest input table -> artifacts/cache/pipeline2_input.json
|
v
[3] build WorkItems -> artifacts/cache/pipeline3_work_items.json
|
v
[4] LLM generate (+ optional judge + retry + cache) -> llm_outputs/* + pipeline4_manifest.json
|
v
[5] export -> artifacts/outputs/output.jsonl + output.psv + pipeline5_manifest.json
|
v
[6] report -> artifacts/reports/report.md + report.html + pipeline6_manifest.json
```

### 1.2 Folder responsibilities

- `functions/batch/`
  - **Pipeline entrypoints only** (thin orchestration)
  - Each pipeline is runnable independently
- `functions/core/`
  - Deterministic domain logic: context building, normalization, schema post-processing, export output core, etc.
- `functions/io/`
  - Deterministic writers, readers, path helpers
- `functions/utils/`
  - Typed config loading, logging, hashing helpers, Unicode hardening
- `functions/llm/`
  - Prompt loading/rendering
  - LLM runner (client adapters, retry, cache, schema validation, judge workflow)
- `prompts/`
  - YAML prompt templates (schema generation, generation prompt, judge prompt, etc.)
- `schema/`
  - `llm_schema.py`: runtime validation schema (Pydantic v2)
  - `llm_schema.txt`: prompt-ready JSON schema contract for LLM injection
- `artifacts/`
  - `cache/`: canonical machine artifacts + manifests + cache entries
  - `outputs/`: user-facing deliverables (jsonl/psv)
  - `reports/`: human-facing report (md/html)
- `scripts/`
  - `run_pipeline_X_force.py`: reproducible real execution scripts per pipeline
- `tests/`
  - `pytest` suite covering core utilities and pipeline behavior

---

## 2) Design Principles (Non-negotiables)

1. **Pipelines are orchestration-only**
   - No business logic in pipeline entrypoints.
2. **Artifacts are the source of truth**
   - Every stage writes inspectable output.
   - No hidden cleanup, no implicit deletion.
3. **Determinism everywhere**
   - Stable ordering, stable serialization, stable hashing, stable IDs.
4. **Schema is the contract**
   - Runtime validation schema (`llm_schema.py`) is authoritative.
   - Prompt schema (`llm_schema.txt`) is derived from runtime schema, not hand-written.
5. **LLM output never bypasses validation**
   - Strict Pydantic v2 validation with `extra="forbid"`.
6. **Judge does not poison cache**
   - Only judge-approved outputs are persisted as "final success artifacts".
7. **Forward-compatibility**
   - Schema/prompt changes produce new cache IDs and do not corrupt old runs.
8. **Manifest-first traceability**
   - Each pipeline writes a manifest summarizing counts, modes, and anomalies.

---

## 3) Pipeline 0 — Schema (Python) Ensure

**File:** `functions/batch/pipeline_0_schema_ensure.py`  
**Outputs:** `schema/llm_schema.py` (+ archived previous versions)

### 3.1 Purpose

Ensure a valid, importable **Pydantic v2 schema module** (`schema/llm_schema.py`) exists before any LLM generation.

### 3.2 Responsibilities

- Load typed config from `configs/parameters.yaml`
- Decide:
  - Schema path
  - Archive path (`archived/`)
  - Force regeneration behavior
- Behavior:
  - If schema exists and `force_regenerate=false` → **NO-OP**
  - If missing or `force_regenerate=true`:
    - Archive existing schema (timestamped)
    - Generate schema via LLM prompt (`prompts/schema_auto_py_generation.yaml`)
    - Strip markdown code fences safely
    - Post-process code deterministically:
      - Ensure `ConfigDict` import exists
      - Inject `model_config = ConfigDict(extra="forbid")` into `BaseModel` subclasses when missing
      - Ensure `__all__` contains required exports
    - Validate:
      - file is syntactically valid Python
      - module import succeeds
      - at least one `pydantic.BaseModel` exists

### 3.3 Guarantees

- Schema is syntactically valid Python
- Schema is importable at runtime
- Schema is strict-ready:
  - `ConfigDict(extra="forbid")` applied to models
  - extra fields rejected deterministically

### 3.4 Real Execution Evidence (2026-02-17)

- LLM call succeeded (HTTP 200)
- Wrote schema → `schema/llm_schema.py` (`bytes=2320`)
- Validated importable + BaseModel found
- Return code: `0`

---

## 4) Pipeline 1 — Schema (Text / JSON) Ensure

**File:** `functions/batch/pipeline_1_schema_txt_ensure.py`  
**Outputs:** `schema/llm_schema.txt` (+ archived previous versions)

### 4.1 Purpose

Convert the Python Pydantic schema into **prompt-ready JSON schema text** (`schema/llm_schema.txt`) for injection into prompts as `{llm_schema}`.

This avoids showing Python code to the LLM and guarantees the prompt contract matches runtime validation.

### 4.2 Responsibilities

- Load schema module from Pipeline 0 (`schema/llm_schema.py`)
- Identify the target model(s) to generate JSON schema from
- Generate JSON schema via Pydantic v2 introspection
- Enforce strict JSON schema preferences:
  - `additionalProperties=false`
  - required fields are explicit
  - field descriptions included when present
- Behavior:
  - If txt exists and `force_regenerate=false` → **NO-OP**
  - Else:
    - Archive existing txt (timestamped)
    - Regenerate from `.py`

### 4.3 Guarantees

- Output is valid JSON (`json.loads()` succeeds)
- Prompt schema aligns with runtime validation schema used in Pipeline 4

### 4.4 Real Execution Evidence (2026-02-17)

- Archived existing schema txt → `archived/llm_schema_20260217_052007.txt`
- Wrote schema txt → `schema/llm_schema.txt` (`bytes=3661`)
- Return code: `0`

---

## 5) Pipeline 2 — Input Ingestion (Raw Table → JSON Snapshot)

**File:** `functions/batch/pipeline_2_ingest_input.py`  
**Outputs:** `artifacts/cache/pipeline2_input.json`

### 5.1 Purpose

Ingest a single raw input table deterministically and write a canonical JSON snapshot.
This snapshot becomes the stable input reference for all downstream pipelines.

### 5.2 Supported input formats

- `csv` ✅ (real execution verified)
- `psv/tsv` (implementation supported; delimiter configurable)
- `xlsx` (implementation supported)

### 5.3 Canonical output schema (high-level)

`artifacts/cache/pipeline2_input.json`

```json
{
  "meta": {
    "input_path": "...",
    "input_format": "...",
    "n_rows": 50,
    "n_cols": 5,
    "columns": ["..."]
  },
  "n_rows": 50,
  "n_cols": 5,
  "columns": ["..."],
  "rows": [{ "...": "..." }],
  "records": [{ "...": "..." }]
}
```

Notes:

- `rows` and `records` are currently duplicated aliases for compatibility/readability.
- All values ingested as **strings** for determinism across pandas/openpyxl quirks.

### 5.4 Responsibilities

- Read the input table
- Normalize values deterministically via `clean_dataframe`:
  - Trim whitespace
  - Collapse internal newlines/tabs into spaces where required
  - Convert null tokens → empty string (`None`, `NaN`, `n/a`, `NULL`, `[None]`, etc.)
  - Unescape common delimited-file escape sequences (e.g., `foo\,bar` → `foo,bar`)
- Persist JSON via deterministic writer (`functions/io/writers.py`)

### 5.5 Guarantees

- Deterministic ingestion → stable snapshots across runs
- JSON-serializable output (no DataFrames)
- Format-agnostic reading is config-driven

### 5.6 Real Execution Evidence (2026-02-17)

- Wrote `artifacts/cache/pipeline2_input.json` (`bytes=40017`)
- Return code: `0`

---

## 6) Pipeline 3 — Work Items (Context Construction → WorkItem Artifacts)

**File:** `functions/batch/pipeline_3_build_requests.py`  
**Outputs:** `artifacts/cache/pipeline3_work_items.json` (and optional group-context artifacts depending on config)

### 6.1 Purpose

Convert the ingested input snapshot into deterministic **WorkItems** representing the unit of LLM work.

Pipeline 3 is the bridge between raw input tables and LLM generation.

### 6.2 Supported execution modes

1. **Row-wise** (no grouping)
   - 1 input row → 1 WorkItem

2. **Group output** (aggregated)
   - 1 group → 1 WorkItem
   - LLM sees all group rows together
   - Output is group-level

3. **Row output with group context** (shared context)
   - Group context built once
   - Each row → 1 WorkItem
   - Each WorkItem references shared group context

### 6.3 Group context de-duplication (Implemented)

For large-scale grouping (e.g., 20,000 rows with `row_output_with_group_context`),
repeating the group context in every WorkItem bloats artifacts.

We implemented deterministic **group-context de-dup**:

- Each unique group context stored **once**
- WorkItems reference contexts by stable `group_context_id` (SHA1 hash of context content)
- Preserves determinism while reducing artifact size significantly

### 6.4 Context builder features (deterministic)

Implemented in `functions/core/context_builder.py`:

- Column selection: `all` (default) / `include` / `exclude`
- KV ordering: `input_order` (default) / `alpha`
- Row template rendering: supports `{__ROW_KV_BLOCK__}` placeholder
- Truncation controls: `truncate_field_chars` (per-field) / `max_context_chars` (whole-context)
- Grouping controls: `group_output` / `row_output_with_group_context` / `max_rows_per_group` cap
- Stable IDs: `work_id` stable across runs; `group_context_id` stable as SHA1(context string)

### 6.5 Guarantees

- WorkItem ordering deterministic
- Group ordering is "first-seen in input order"
- Row ordering within group stable
- Truncation recorded in meta (no silent truncation)
- Hash IDs stable across repeated runs

### 6.6 Real Execution Evidence (2026-02-17)

- Wrote `artifacts/cache/pipeline3_work_items.json` (`bytes=44138`)
- Produced **5 WorkItems** (group-level) for **5 role groups**
- Return code: `0`

---

## 7) Grouping Modes (Pipelines 3–5) — Behavior Summary

The framework supports three core modes:

### 7.1 Row-wise (No Grouping)

```yaml
grouping:
  enabled: false
```

- 1 input row → 1 WorkItem → 1 output
- No shared context

### 7.2 Group output (Aggregated)

```yaml
grouping:
  enabled: true
  column: "Course Code"
  mode: group_output
  max_rows_per_group: 50
```

- 1 group → 1 WorkItem → 1 output
- LLM sees all rows in group

### 7.3 Row output with group context (Shared Context)

```yaml
grouping:
  enabled: true
  column: "Role Track Example Name"
  mode: row_output_with_group_context
  max_rows_per_group: 50
```

- Group context built once
- Each row produces its own WorkItem output
- Dedup applies automatically

---

## 8) Pipeline 4 — LLM Batch Generation (+ Optional Judge)

**File:** `functions/batch/pipeline_4_llm_generate.py`  
**Outputs:** `artifacts/cache/llm_outputs/*`, `artifacts/cache/llm_failures/*`, `artifacts/cache/pipeline4_manifest.json`

### 8.1 Purpose

Execute deterministic LLM generation per WorkItem with:

- strict schema validation (Pydantic v2)
- optional judge validation (auto-retry with feedback)
- artifact-level caching keyed by deterministic `cache_id`
- comprehensive manifest for traceability

### 8.2 Core workflow

1. Load WorkItems (+ group contexts if dedup mode)
2. Compute deterministic `cache_id` per WorkItem
3. Cache pre-scan: if success artifact exists and cache enabled → skip
4. Run generation prompt
5. Validate output strictly against runtime schema
6. Optional judge: if judge fails, append feedback and retry; only judge-approved outputs persisted
7. Write success artifacts + failure artifacts (attempt-scoped)
8. Write `pipeline4_manifest.json`

### 8.3 Parallel execution

- Uses `llm.max_workers` (ThreadPoolExecutor)
- Supports large-scale batch runs without changing core logic
- Determinism preserved by stable input ordering + stable cache IDs

### 8.4 Cache strategy (artifact-level)

Deterministic `cache_id` computed as SHA1 over:

```
work_id + prompt_sha + schema_sha + model_name + temperature + judge_enabled + judge_prompt_sha
```

### 8.5 Real Execution Evidence (2026-02-17)

- `n_items=5 judge=True cache=True force=False model=gemini-3-flash-preview temp=1.000 retries=5 workers=10`
- Pre-scan: `cache_skips=0 will_run=5/5`
- `extra_forbidden` on attempt 1/5 → recovered via retry
- Completion: `ok=5 fresh_ok=5 fail=0 cache_skips=0 elapsed=279.64s`
- Return code: `0`

---

## 9) Pipeline 5 — Export Outputs (JSONL + PSV)

**File:** `functions/batch/pipeline_5_export_outputs.py`  
**Core logic:** `functions/core/export_outputs.py`  
**Outputs:** `artifacts/outputs/output.jsonl`, `artifacts/outputs/output.psv`, `artifacts/cache/pipeline5_manifest.json`

### 9.1 Purpose

Export Pipeline 4 validated artifacts into user-friendly, analysis-ready formats:

- `jsonl` — full canonical records (always complete, never lossy)
- `psv` — thin, spreadsheet-friendly table with stable column ordering

### 9.2 Architecture

Core logic is extracted to `functions/core/export_outputs.py` (zero file IO, fully testable).
`pipeline_5_export_outputs.py` is thin orchestration only: paths + config + IO.

Key components in core:
- `build_pipeline2_index()` — loads input columns + row index
- `build_group_context_index()` — loads group contexts by ID
- `build_export_records()` — main builder with injectable `read_json_func` for testability
- `flatten_for_psv()` — produces flat rows with collision-safe parsed field naming
- `compute_psv_column_order()` — deterministic column ordering

### 9.3 Thin PSV (default)

Config (`configs/parameters.yaml`):

```yaml
outputs:
  drop_heavy_columns: true  # default
```

Heavy columns excluded from PSV by default (still present in JSONL):

- `group_rows_json`
- `group_context_id`
- `group_context`
- `group_context_meta_json`
- `questions_json`

JSONL always contains full records and is the canonical artifact.

### 9.4 PSV column ordering (deterministic)

1. Input columns (source order)
2. Group helper columns (if enabled; light trace only by default)
3. Parsed fields (alphabetical)
4. Judge + meta columns (fixed order)

### 9.5 PSV safety guarantees

All cell values pass through `sanitize_psv_value()`:

| Character | Escaped as |
|-----------|------------|
| Newline `\n` | `\\n` |
| Tab `\t` | `\\t` |
| Pipe `\|` | `\\|` |
| `None` | `""` |
| int/float/bool | unchanged |

Downstream consumers must unescape these sequences when reading cell values.

### 9.6 Parsed field collision resolution

If a parsed schema field name collides semantically with an input column header (e.g., parsed `question_id` vs input `Question ID`), the parsed field is automatically prefixed with `parsed_` to preserve both columns without silent overwrite.

Collision detection uses header normalization: `"Question ID"` → `"question_id"` (lowercase, non-alphanumeric → `_`).

### 9.7 Export mode verified: `row_output_expanded`

- Pipeline 4 produced **group-level** outputs (5 groups)
- Pipeline 5 expanded them into **row-level** outputs (50 rows)

### 9.8 Outputs (Real Run — 2026-02-17)

- `artifacts/outputs/output.jsonl` — **rows=50** (full records)
- `artifacts/outputs/output.psv` — **rows=50, cols=22, sep=|** (thin)
- `artifacts/cache/pipeline5_manifest.json`

### 9.9 Guarantees

- JSONL is always full (canonical, never lossy)
- PSV is thin by default (26% smaller than full)
- Stable column ordering (deterministic)
- Manifest includes full diagnostic counts (missing outputs, out-of-bounds, selected question missing)

### 9.10 Real Execution Evidence (2026-02-17)

- Wrote JSONL: `artifacts/outputs/output.jsonl` (`rows=50`)
- Wrote PSV: `artifacts/outputs/output.psv` (`sep=| rows=50 cols=22`)
- Manifest written: `artifacts/cache/pipeline5_manifest.json`
- `mode=row_output_expanded exported=50 drop_heavy=True`
- Return code: `0`

---

## 10) Pipeline 6 — Report Generation (MD + HTML)

**File:** `functions/batch/pipeline_6_write_report.py`  
**Outputs:** `artifacts/reports/report.md`, `artifacts/reports/report.html`, `artifacts/cache/pipeline6_manifest.json`

### 10.1 Purpose

Generate a human-readable report that summarizes:

- run metadata, config, and artifact paths
- volume stats and distributions
- judge performance and outcome stats
- samples for spot-checking
- data quality checks and structural sanity counts

### 10.2 Report content verified (2026-02-17)

- Run Summary: `n_records=50`, `judge_enabled=True`, `judge_pass=50`, `judge_fail=0`
- Judge scores: `min=92`, `avg=93.8`, `max=95`
- Counts by Role: 10 each (Consultant / Data Engineer / Data Scientist / Product Owner / Software Developer)
- Counts by Question Set: A=25, B=25
- Counts by Question Type: Generic=10, Personality=10, Specific=30
- Data Quality: `missing_input_count=0`, `missing_meta_count=0`, `invalid_judge_count=0`

### 10.3 Known report improvements (identified)

- Judge "Top Reasons" currently semantically duplicated (minor phrasing variants counted separately).
  - Next improvement: normalize/canonicalize reasons before counting.
- Report should explicitly distinguish:
  - **source_mode** (Pipeline 4 output mode; group vs row)
  - **export_mode** (Pipeline 5 export mode; row-expanded vs group-level)

### 10.4 Real Execution Evidence (2026-02-17)

- Wrote `artifacts/reports/report.md`
- Wrote `artifacts/reports/report.html`
- Manifest written: `artifacts/cache/pipeline6_manifest.json`
- Return code: `0`

---

## 11) Foundational Systems (Stable)

### 11.1 Prompt System

**File:** `functions/llm/prompts.py`

- YAML prompt loading (file-based)
- Deterministic rendering with safe placeholder replacement
- Unicode hardening (NBSP/BOM/narrow NBSP normalization)
- Strict missing-placeholder errors (clear KeyError)
- Safe injection: injected values may contain `{` and `}` without breaking rendering

### 11.2 Deterministic Writers

**File:** `functions/io/writers.py`

- Deterministic JSON / JSONL / delimited writing: UTF-8, `ensure_ascii=False`, `sort_keys=True`
- Ensures parent directories exist; logs bytes written
- Intentionally thin: no schema enforcement, no DataFrame conversion

### 11.3 Config System

**File:** `functions/utils/config.py`

- Typed YAML configs using Pydantic v2
- Unicode hardening before YAML parse
- Strict validation with actionable error messages
- Forward-compatible defaults (optional blocks)

---

## 12) Current State (Green)

End-to-end verified:

- ✅ Pipelines 0–6 finished with return code `0`
- ✅ Schema created and prompt schema generated
- ✅ WorkItems built deterministically
- ✅ LLM generation succeeded with strict validation (+ judge enabled)
- ✅ Outputs exported to JSONL (full, 50 rows) + PSV (thin, 50 rows × 22 cols)
- ✅ Report generated to MD + HTML
- ✅ All exported records judge-passed (50/50)
- ✅ 236 tests passing (221 + 15)

---

## 13) Foundation Artifacts (Known Sizes for Current Example)

| Layer           | File                                          | Purpose                               | Size / Count   |
| --------------- | --------------------------------------------- | ------------------------------------- | -------------- |
| Validation      | `schema/llm_schema.py`                        | Runtime validation (Pydantic)         | 2,320 bytes    |
| Prompt Contract | `schema/llm_schema.txt`                       | LLM contract (JSON schema text)       | 3,661 bytes    |
| Input Canon     | `artifacts/cache/pipeline2_input.json`        | Deterministic input snapshot          | 40,017 bytes   |
| Work Units      | `artifacts/cache/pipeline3_work_items.json`   | WorkItems                             | 44,138 bytes   |
| Manifest        | `artifacts/cache/pipeline4_manifest.json`     | LLM execution summary                 | (varies)       |
| Outputs Cache   | `artifacts/cache/llm_outputs/*.json`          | Validated output artifacts (group)    | 5 files        |
| Deliverables    | `artifacts/outputs/output.jsonl`              | Row-expanded outputs (full, canonical)| 50 rows        |
| Deliverables    | `artifacts/outputs/output.psv`                | Row-expanded outputs (thin PSV table) | 50×22 cells    |
| Export Manifest | `artifacts/cache/pipeline5_manifest.json`     | Export accounting + mode summary      | ~1,571 bytes   |
| Report          | `artifacts/reports/report.md` / `report.html` | Run report                            | (varies)       |
| Report Manifest | `artifacts/cache/pipeline6_manifest.json`     | Report traceability                   | 1,373 bytes    |

---

## 14) Next Steps (Post 0–6)

### A) Report polish / correctness upgrades (Pipeline 6)

- Canonicalize judge reasons before counting ("Top Reasons" de-dup)
- Report both `source_mode` (Pipeline 4 output mode) and `export_mode` (Pipeline 5 export mode)
- Add minimal "data checks" section:
  - PSV row count matches expected
  - Required columns present
  - Any selected question missing counts
  - Cache hit rate summary if rerun

### B) Export ergonomics (Pipeline 5)

- Optional: export per-group files (split by `group_key`)
- Optional: add stable `run_id` in every exported row for cross-run joining

### C) Scale readiness

- Validate large-N behavior: group context dedup size reductions, cache hit rates, manifest growth
- Add a benchmark script: token usage estimate, elapsed time breakdown, retries and failure modes, cache effectiveness

---

## 15) Changelog

### 2026-02-17 — Pipeline 5 Refactor: Thin PSV + Core Extraction ✅

- **Core logic extracted** to `functions/core/export_outputs.py` (zero file IO, fully testable)
  - `pipeline_5_export_outputs.py` is now thin orchestration only
- **Thin PSV by default** (`drop_heavy_columns: true`):
  - PSV reduced from 31 cols → 22 cols (26% smaller)
  - 5 heavy columns dropped from PSV: `group_rows_json`, `group_context_id`, `group_context`, `group_context_meta_json`, `questions_json`
  - JSONL always remains full (canonical, never lossy)
- **PSV safety hardened** (`sanitize_psv_value`):
  - Added `\|` pipe escaping (structural correctness for cells containing pipe characters)
  - Existing `\n` and `\t` escaping preserved
- **Parsed field collision resolution**:
  - Semantic collision detection via header normalization (`"Question ID"` → `"question_id"`)
  - Colliding parsed fields auto-prefixed with `parsed_` (e.g., `parsed_question_id`)
  - Prevents silent column overwrites
- **Double-stringify prevention**:
  - `judge_reasons_json` and `questions_json` check if value is already a string before calling `json_stringify_if_needed`
- **PSV output validated**: 50 rows × 22 cols, all column splits correct, rubric `\n` sequences escaped properly
- **Test suite**: 236 tests passing (221 + 15), 0 failures

### 2026-02-17 — Pipelines 5–6 Completed ✅

- Pipeline 5: Exported `output.jsonl` + `output.psv`
  - Verified `row_output_expanded` mode: `group_outputs=5 → expanded_rows=50`
  - Manifest counts: `missing_out=0 row_oob=0 selected_q_missing=0`
- Pipeline 6: Generated `report.md` + `report.html`
  - Captured judge statistics and distributions
  - Recorded quality summary counts

### 2026-02-17 — End-to-End Real Execution ✅

- Pipelines 0–6 all `rc=0`
- Pipeline 4 strict schema validation warning recovered via retry (final failures 0)
- Delivered analysis-ready exports + report artifacts

---

**Status: Framework v1 is operational and producing real deliverables end-to-end (schema → generation → exports → report).**