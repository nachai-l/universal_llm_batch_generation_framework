# Universal LLM Batch Generation Framework ‚Äî Progress Summary

> Status: **Pipelines 0‚Äì4 completed, tested, and real-execution verified** ‚úÖ  
> Date: **2026-02-16** (Asia/Tokyo)

---

## Overall Architecture Direction

We are building a **universal, task-agnostic LLM batch generation framework** with:

* Thin, explicit **pipeline entrypoints** (`functions/batch/pipeline_X_*.py`)
* All real logic isolated in:
  * `functions/core/` (domain logic)
  * `functions/io/` (file I/O)
  * `functions/utils/` (config, logging, helpers)
  * `functions/llm/` (prompt/runner/client adapters)
* Deterministic, testable behavior
* Strong forward-compatibility (schema evolution, prompt evolution, grouping, context)
* Artifact-first traceability (everything inspectable; no hidden cleanup)

Each pipeline:

* Is independently runnable
* Has a real execution script (`scripts/run_pipeline_X_force.py`)
* Is fully covered by pytest
* Produces inspectable artifacts (no magic, no implicit deletion)

---

## Pipeline 0 ‚Äî Schema (Python) Ensure

**File:** `functions/batch/pipeline_0_schema_ensure.py`

### Purpose

Ensure a valid, importable **Pydantic v2 schema module** (`schema/llm_schema.py`) exists before any LLM generation.

### Responsibilities

* Load typed config from `configs/parameters.yaml`
* Determine schema path + archive directory
* Behavior:
  * If schema exists and `force_regenerate=false` ‚Üí **NO-OP**
  * If missing or `force_regenerate=true`:
    * Archive existing schema (timestamped)
    * Generate schema via LLM prompt (`prompts/schema_auto_py_generation.yaml`)
    * Strip code fences safely
    * Inject `ConfigDict(extra="forbid")` when missing
    * Validate:
      * Python-importable
      * At least one `pydantic.BaseModel`

### Guarantees

* Schema is syntactically valid Python
* Schema is importable at runtime
* Schema is strict-ready (`extra="forbid"`)

### Tests

**File:** `tests/test_pipeline_0_schema_ensure.py` (15 tests)

Covers:

* Fence extraction priority and fallback
* Import validation and failure modes
* Generate vs force vs noop behavior
* Archive correctness
* Forward-compatible prompt path usage
* Cache flags behavior

### Real Execution Evidence (2026-02-16)

Log highlights:

* Archived existing schema ‚Üí `archived/llm_schema_20260216_164440.py`
* Regenerated schema via prompt ‚Üí `schema/llm_schema.py` (`bytes=1711`)
* Validated importable + BaseModel found
* Return code: `0`

---

## Pipeline 1 ‚Äî Schema (Text / JSON) Ensure

**File:** `functions/batch/pipeline_1_schema_txt_ensure.py`

### Purpose

Convert the Python Pydantic schema into **prompt-ready JSON schema text** (`schema/llm_schema.txt`) for direct injection into generation prompts as `{llm_schema}`.

This avoids ever showing Python code to the LLM.

### Output

* `schema/llm_schema.txt` (valid JSON)

Example structure:

```json
{
  "title": "LLM Schema",
  "type": "object",
  "models": {
    "LLMOutput": { "...": "..." },
    "JudgeResult": { "...": "..." }
  }
}
````

### Responsibilities

* Load schema `.py` from Pipeline 0
* Build JSON schema via Pydantic v2 introspection
* Enforce:

  * `additionalProperties=false`
  * required fields
  * field descriptions (when available)
* Behavior:

  * If txt exists and `force_regenerate=false` ‚Üí **NO-OP**
  * Else:

    * Archive existing txt (timestamped)
    * Regenerate from `.py`

### Guarantees

* Output is valid JSON (`json.loads()` always succeeds)
* Prompt schema aligns with runtime validation schema used in Pipeline 4

### Tests

**Files:**

* `tests/test_pipeline_1_schema_txt_ensure.py`
* `tests/test_schema_text.py`

Covers:

* Generation when txt missing
* No-op behavior
* Archive behavior
* JSON parseability + structural expectations

### Real Execution Evidence (2026-02-16)

Log highlights:

* Archived existing txt ‚Üí `archived/llm_schema_20260216_164441.txt`
* Wrote `schema/llm_schema.txt` (`bytes=2548`)
* Return code: `0`

---

## Pipeline 2 ‚Äî Input Ingestion (Raw Table ‚Üí JSON Snapshot)

**File:** `functions/batch/pipeline_2_ingest_input.py`

### Purpose

Ingest a single raw input table (`csv/psv/tsv/xlsx`) deterministically and write a canonical **JSON snapshot** that becomes the stable input reference for downstream pipelines.

### Output

Default:

* `artifacts/cache/pipeline2_input.json`

High-level structure:

```json
{
  "meta": {
    "input_path": "raw_data/input.csv",
    "input_format": "csv",
    "n_rows": 50,
    "n_cols": 5,
    "columns": ["..."]
  },
  "n_rows": 50,
  "n_cols": 5,
  "columns": ["..."],
  "rows": [{ "...": "..." }],
  "records": [{ "...": "..." }]
}
```

Notes:

* `rows` and `records` are currently duplicated aliases for compatibility/readability.
* All values ingested as **strings** for determinism.

### Responsibilities

* Read supported formats (CSV verified via real execution; others supported by implementation)
* Normalize table cells (via `clean_dataframe`):

  * Trim whitespace
  * Collapse internal newlines/tabs into spaces where required
  * Convert null tokens to empty string (`None`, `NaN`, `n/a`, `NULL`, `[None]`, etc.)
  * Unescape sequences used in delimited files (e.g., `foo\,bar` ‚Üí `foo,bar`)
* Produce JSON-serializable payload
* Persist deterministically via `functions/io/writers.py`

### Guarantees

* Deterministic ingestion ‚Üí stable snapshots across runs
* JSON-serializable output (no DataFrames)
* Format-agnostic reading (config-driven)

### Tests

**Files:**

* `tests/test_ingestions.py`
* `tests/test_pipeline_2_ingest_input.py`

Covers:

* Cell normalization edge cases
* Multiline parsing behavior + post-cleaning flattening
* PSV conversion + delimiter correctness
* Required keys in artifact

### Real Execution Evidence (2026-02-16)

Log highlights:

* Wrote `artifacts/cache/pipeline2_input.json` (`bytes=40017`)
* Return code: `0`

---

## Pipeline 3 ‚Äî Work Items (Context Construction ‚Üí WorkUnit Artifacts)

**File:** `functions/batch/pipeline_3_build_requests.py`

### Purpose

Convert the ingested input snapshot into deterministic **WorkItems** representing the unit of LLM work.

Pipeline 3 is the bridge between raw table data and LLM generation.

### Supported modes

* Row-wise (no grouping): **1 row ‚Üí 1 WorkItem**
* Group output: **1 group ‚Üí 1 WorkItem**
* Row output with group context: **N rows ‚Üí N WorkItems**, each referencing the same group context

### üöÄ Group Context De-Duplication (Implemented)

For large-scale grouping (e.g., 20,000 rows with `row_output_with_group_context`), repeating group context in every WorkItem would bloat artifacts.

We implemented deterministic **group-context de-duplication**:

* Stores each unique group context **once** in a separate artifact
* WorkItems reference contexts via stable `group_context_id` (SHA1 hash)
* Reduces artifact size substantially while preserving determinism

### Output

Default:

* `artifacts/cache/pipeline3_work_items.json` (always)
* `artifacts/cache/pipeline3_group_contexts.json` (only when dedup applies)

Work items structure (dedup mode):

```json
{
  "meta": {
    "grouping": {
      "enabled": true,
      "mode": "row_output_with_group_context",
      "dedupe_group_context": true
    }
  },
  "n_items": 50,
  "n_group_contexts": 5,
  "group_contexts_artifact": "pipeline3_group_contexts.json",
  "items": [
    {
      "work_id": "sha1...",
      "group_key": "Data Scientist",
      "row_index": 0,
      "group_context_id": "sha1...",
      "meta": {
        "mode": "row_output_with_group_context",
        "deduped_group_context": true
      }
    }
  ]
}
```

Group contexts structure:

```json
[
  {
    "group_context_id": "sha1...",
    "group_key": "Data Scientist",
    "context": "full group context string here...",
    "meta": {
      "grouping_column": "Role Track Example Name",
      "n_rows_total": 10,
      "n_rows_used": 10,
      "row_cap_applied": false
    }
  }
]
```

Size comparison (50 rows, 5 groups):

* Without dedup: `pipeline3_work_items.json` ‚âà **257 KB**
* With dedup:

  * `pipeline3_work_items.json` ‚âà **60 KB**
  * `pipeline3_group_contexts.json` ‚âà **21 KB**
  * Total ‚âà **81 KB** (**~68% reduction**)

Projection (20,000 rows):

* Without dedup: **~103 MB**
* With dedup: **~24 MB** (**~77% reduction**)

### Responsibilities

* Read Pipeline 2 artifact deterministically
* Build WorkItems via `functions/core/context_builder.py`:

  * Column selection modes: `all` (default), `include`, `exclude`
  * KV ordering: `input_order` (default), `alpha`
  * Row template: `{__ROW_KV_BLOCK__}` placeholder supported
  * Truncation:

    * per-field (`truncate_field_chars`)
    * whole-context (`max_context_chars`)
  * Grouping:

    * `group_output`
    * `row_output_with_group_context`
    * deterministic row caps (`max_rows_per_group`)
  * De-duplication:

    * Build GroupContext objects once
    * Assign `group_context_id` as SHA1(context content)
    * WorkItems reference by ID
* Persist deterministically via `functions/io/writers.py`

### Guarantees

* WorkItem ordering deterministic
* `work_id` stable across runs (same inputs/config)
* `group_context_id` stable (SHA1 of context content)
* Group ordering is ‚Äúfirst-seen in input order‚Äù
* Row ordering within group stable
* Truncation explicitly recorded in meta
* Dedup deterministic, identical outputs on reruns

### Tests

**Files:**

* `tests/test_context_builder.py`
* `tests/test_pipeline_3_build_requests.py`

Covers:

* Empty input behavior
* Row-wise generation
* Column selection + ordering
* Truncation + meta markers
* Group output first-seen ordering + row caps
* Row output with group context
* Dedup correctness + stable IDs + separate artifact
* Missing grouping column raises `ValueError`

### Real Execution Evidence (2026-02-16)

Log highlights:

* Wrote `pipeline3_group_contexts.json` (`bytes=21265`)
* Wrote `pipeline3_work_items.json` (`bytes=60246`)
* Produced **50 WorkItems + 5 GroupContexts**
* Return code: `0`

---

## Grouping Modes (Pipelines 3‚Äì4) ‚Äî Behavior Summary

The framework supports **three execution modes**:

### 1) Row-wise (No Grouping)

```yaml
grouping:
  enabled: false
```

* 1 input row ‚Üí 1 WorkItem ‚Üí 1 output
* No shared context

Use cases: independent questions, extraction tasks, row-level checks.

### 2) Group Output (Aggregated)

```yaml
grouping:
  enabled: true
  column: "Course Code"
  mode: group_output
  max_rows_per_group: 50
```

* 1 group ‚Üí 1 WorkItem ‚Üí 1 output
* LLM sees all group rows together

Use cases: course summaries, grouped reports.

### 3) Row Output with Group Context (Shared Context)

```yaml
grouping:
  enabled: true
  column: "Role Track Example Name"
  mode: row_output_with_group_context
  max_rows_per_group: 50
```

* Group context built once
* Each row ‚Üí 1 WorkItem ‚Üí 1 output
* Each WorkItem references group context (dedup applies automatically)

Use cases: per-row outputs that must be aware of sibling rows (e.g., ‚Äúquestions must be distinct within role‚Äù).

**Dedup applies when:** `enabled=true` AND `mode=row_output_with_group_context`

---

## Pipeline 4 ‚Äî LLM Batch Generation (+ Optional Judge)

**File:** `functions/batch/pipeline_4_llm_generate.py`

### Purpose

Execute deterministic LLM generation for each WorkItem, optionally run judge validation (with auto-retry), and write **one validated output artifact per accepted WorkItem**.

Pipeline 4 is **artifact-level cached**: `llm_outputs/{cache_id}.json` is the canonical cache entry.

### Core workflow

* Load WorkItems + GroupContexts
* Reconstruct full context from `group_context_id` map (dedup aware)
* Compute deterministic `cache_id` per WorkItem
* Cache pre-scan to skip work when outputs already exist
* Run generation prompt + strict schema validation
* Optional judge prompt:

  * If judge fails: append feedback and retry (up to configured retries)
  * Only judge-approved outputs are persisted as ‚Äúfinal‚Äù
* Write success/failure artifacts
* Emit comprehensive `pipeline4_manifest.json`

### Parallel execution

Pipeline 4 supports parallelism via `llm.max_workers` (ThreadPoolExecutor).

* `max_workers=1` ‚Üí sequential execution (baseline behavior)
* `max_workers>1` ‚Üí parallel execution (I/O bound LLM calls)

Real run example shows **workers=10** with correct progress accounting and deterministic artifact outputs.

### Outputs

**Artifacts:**

* `artifacts/cache/llm_outputs/{cache_id}.json` ‚Äî success artifacts (validated + optional judge)
* `artifacts/cache/llm_failures/{cache_id}__a{attempt}.json` ‚Äî failure artifacts (attempt-scoped)
* `artifacts/cache/pipeline4_manifest.json` ‚Äî summary/traceability

### Success artifact schema (high-level)

```json
{
  "meta": {
    "cache_id": "sha1...",
    "work_id": "sha1...",
    "group_key": "Product Owner",
    "row_index": 47,
    "group_context_id": "sha1...",
    "model": "gemini-3-flash-preview",
    "temperature": 1.0,
    "generation_prompt_path": ".../generation.yaml",
    "judge_enabled": true,
    "judge_prompt_path": ".../judge.yaml",
    "attempt_outer": 1,
    "created_at_utc": "2026-02-16T..."
  },
  "parsed": { "... validated output ..." },
  "judge": { "... judge result ..." }
}
```

### Manifest schema (high-level)

```json
{
  "meta": {
    "pipeline": 4,
    "generated_at_utc": "2026-02-16T...",
    "model_name": "gemini-3-flash-preview",
    "temperature": 1.0,
    "judge_enabled": true,
    "prompt_sha": "sha1...",
    "schema_sha": "sha1...",
    "judge_prompt_sha": "sha1...",
    "elapsed_sec": 64.42,
    "verbose": 3,
    "progress_every_n": 3,
    "cache_skips_pre_scan": 40,
    "cache_skips_observed": 40,
    "max_workers": 10,
    "execution_mode": "parallel"
  },
  "counts": {
    "n_total": 50,
    "n_success": 50,
    "n_fail": 0,
    "n_cache_skipped": 40,
    "n_to_run": 10
  },
  "outputs": {
    "outputs_dir": "artifacts/cache/llm_outputs",
    "failures_dir": "artifacts/cache/llm_failures",
    "success_files": ["..."],
    "failure_files": []
  }
}
```

### Cache strategy (artifact-level)

Deterministic `cache_id` computed as SHA1 over:

```
work_id +
prompt_sha +
schema_sha +
model_name +
temperature +
judge_enabled +
judge_prompt_sha
```

Behavior:

* Cache hit (output file exists, `cache.enabled=true`, `cache.force=false`) ‚Üí skip LLM call
* Cache miss ‚Üí run generation + validation (+ judge) and write output
* `cache.force=true` ‚Üí re-run and overwrite output

Why artifact-level cache:

* Runner-level caching would allow judge-failed outputs to persist as ‚Äúfinal‚Äù
* Artifact-level ensures only **validated + accepted** outputs are cached

### Real execution evidence (2026-02-16)

Commands run:

```bash
pytest -q
pytest tests/test_pipeline_0_schema_ensure.py -vv
python scripts/run_pipeline_0_force.py
python scripts/run_pipeline_1_force.py
python scripts/run_pipeline_2_force.py
python scripts/run_pipeline_3_force.py
python scripts/run_pipeline_4_force.py
```

Results:

* ‚úÖ **206 passed** in **0.90s**
* ‚úÖ Pipelines 0‚Äì3 real execution verified (all rc=0)
* ‚úÖ Pipeline 4 real execution verified (parallel + cache-aware):

  * Start: `n_items=50 judge=True cache=True force=False model=gemini-3-flash-preview temp=1.0 retries=3 workers=10 verbose=3`
  * Pre-scan: `cache_skips=40 will_run=10/50 (mode=parallel)`
  * Completion: `ok=50 fresh_ok=10 fail=0 cache_skips=40 elapsed=64.42s workers=10`
* Observed one validation warning during run:

  * `LLM output invalid (attempt 1/3) ... extra_forbidden`
  * Run recovered via retry path; final failures = **0**

---

## Supporting Modules (Pipeline 4)

| Module                                  | Purpose                                  | Key Functions                                                   |
| --------------------------------------- | ---------------------------------------- | --------------------------------------------------------------- |
| `functions/core/llm_batch_engine.py`    | Generation + optional judge + retry loop | `generate_with_optional_judge()`                                |
| `functions/core/llm_batch_storage.py`   | Cache ID + pre-scan + failure writing    | `stable_cache_id()`, `pre_scan_cache()`, `write_failure_json()` |
| `functions/core/llm_artifacts_cache.py` | Parse Pipeline 3 artifacts               | `parse_pipeline3_items()`, `load_group_context_map()`           |
| `functions/core/schema_runtime.py`      | Dynamic schema loading                   | `load_schema_module()`, `resolve_schema_models()`               |
| `functions/utils/hashing.py`            | Deterministic fingerprints               | `sha1_text()`, `sha1_file()`                                    |
| `functions/utils/verbosity.py`          | Logging controls                         | `clamp_verbose()`, `VerbosityLogger`, `item_log_every_n()`      |
| `functions/utils/paths.py`              | CWD-independent paths                    | `repo_root_from_parameters_path()`, `resolve_path()`            |

---

## Foundational Systems

### Prompt System

**File:** `functions/llm/prompts.py`

* YAML prompt loading (file-based)
* Deterministic rendering with safe placeholder replacement
* Unicode hardening (NBSP/BOM/narrow NBSP)
* Strict missing-placeholder errors (clear KeyError)
* Safe JSON/text injection: injected values may contain `{` and `}` without breaking rendering

### Deterministic Writers

**File:** `functions/io/writers.py`

* Deterministic JSON/JSONL/CSV writing:

  * UTF-8
  * `ensure_ascii=False`
  * `sort_keys=True`
  * pretty indent
  * ensures parent dirs exist
  * logs bytes written
* Intentionally thin: no schema enforcement, no DataFrame conversion

### Config System

**File:** `functions/utils/config.py`

* Typed YAML configs using Pydantic v2
* Unicode hardening before YAML parse
* Strict validation with actionable errors
* Forward-compatible defaults (optional blocks)

---

## Current State (Green)

```bash
pytest -q  # 206 passed in 0.90s
python scripts/run_pipeline_X_force.py  # X = 0,1,2,3,4
```

Verified outcomes:

* ‚úÖ Pipeline 0: schema regenerated + archived
* ‚úÖ Pipeline 1: schema txt regenerated + archived
* ‚úÖ Pipeline 2: deterministic ingest snapshot written
* ‚úÖ Pipeline 3: WorkItems + GroupContexts written (dedup active; size reduced)
* ‚úÖ Pipeline 4: parallel + cache-aware generation succeeded (0 failures)
* ‚úÖ End-to-end artifacts produced and inspectable

---

## Foundation Artifacts (Known Sizes for Current 50-row Example)

| Layer          | File                                            | Purpose                         | Size              |
| -------------- | ----------------------------------------------- | ------------------------------- | ----------------- |
| Validation     | `schema/llm_schema.py`                          | Runtime validation (Pydantic)   | 1,711 bytes       |
| Prompt         | `schema/llm_schema.txt`                         | LLM contract (JSON schema text) | 2,548 bytes       |
| Input Canon    | `artifacts/cache/pipeline2_input.json`          | Deterministic input snapshot    | 40,017 bytes      |
| Group Contexts | `artifacts/cache/pipeline3_group_contexts.json` | Deduplicated contexts           | 21,265 bytes      |
| Work Units     | `artifacts/cache/pipeline3_work_items.json`     | WorkItems + refs                | 60,246 bytes      |
| Outputs        | `artifacts/cache/llm_outputs/*.json`            | Validated output artifacts      | 50 files (varies) |
| Failures       | `artifacts/cache/llm_failures/`                 | Failure artifacts               | 0 files           |
| Manifest       | `artifacts/cache/pipeline4_manifest.json`       | Execution summary               | ~3 KB             |

---

## Design Principles Reinforced

* Pipelines are orchestration-only
* Core logic is reusable and testable
* Artifacts are explicit and inspectable
* Prompts never depend on Python code
* Schema is the single source of truth (runtime validation + prompt schema)
* Deterministic everywhere (ingestion, grouping, hashing, caching)
* Dedup maintains determinism while enabling enterprise scale
* Artifact-level cache ensures only accepted outputs persist
* Judge validation integrated with auto-retry, without poisoning cache

---

## Next Steps ‚Äî Pipelines 5‚Äì6 (Planned)

### Pipeline 5 ‚Äî Export Outputs

* Read validated `llm_outputs/*.json`
* Join outputs back to input metadata (work_id, row_index, group_key, group context linkage)
* Write to configured formats (PSV/JSONL/CSV) with:

  * stable column ordering
  * deterministic filenames
  * explicit schema for export columns

### Pipeline 6 ‚Äî Report Generation

* Generate MD/HTML report
* Include:

  * run metadata + config snapshot
  * summary stats (success/fail/cache/latency)
  * traceability pointers to artifacts
  * sample outputs for spot-checking

---

## Changelog

### 2026-02-16 ‚Äî Pipeline 4 Completed ‚úÖ

**Key features added/verified:**

* Artifact-level caching (`llm_outputs/{cache_id}.json` is canonical cache)
* Cache pre-scan (reports expected hit rate before running)
* Dedup-aware context reconstruction (`group_context_id` ‚Üí full context)
* Strict schema validation + retry path
* Optional judge + auto-retry on judge failure (cache only accepted outputs)
* Parallel execution via `llm.max_workers`

**Real run evidence (50 items):**

* `workers=10` (parallel)
* Pre-scan: **40 cache hits**, **10 to run**
* Completed: **50 ok**, **0 fail**
* Elapsed: **64.42s**
* Observed validation warning recovered via retry; final failures=0

### 2026-02-16 ‚Äî Pipeline 3 Group Deduplication ‚úÖ

* Dedup group context in `row_output_with_group_context`
* `GroupContext` objects stored separately
* WorkItems reference by stable SHA1 IDs
* Observed size reduction: **257 KB ‚Üí 81 KB** (50 rows)

### 2026-02-16 ‚Äî Pipelines 0‚Äì2 Foundation ‚úÖ

* Pipeline 0: schema auto-generation + strict hardening + archival
* Pipeline 1: JSON schema text extraction + archival
* Pipeline 2: deterministic ingestion snapshot artifact

---

**Status: Ready to proceed to Pipeline 5 (Export Outputs).**

